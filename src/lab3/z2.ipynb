{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T00:01:13.611654Z",
     "start_time": "2026-02-05T00:01:04.066292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "model_bert = AutoModel.from_pretrained(\"allegro/herbert-base-cased\").to(device)\n",
    "model_bert.eval()\n",
    "\n",
    "tokenizer_polka = AutoTokenizer.from_pretrained(\"eryk-mazus/polka-1.1b\")\n",
    "model_polka = AutoModelForCausalLM.from_pretrained(\"eryk-mazus/polka-1.1b\").to(device)\n",
    "model_polka.eval()\n",
    "\n",
    "print(\"Models loaded\")"
   ],
   "id": "9b780569cf18098",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T00:04:40.892040Z",
     "start_time": "2026-02-05T00:04:40.885804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_bert_vect(L):\n",
    "    txt = ' '.join(L)\n",
    "    input_ids = tokenizer_bert(txt, return_tensors='pt')['input_ids'].to(device)\n",
    "    output = model_bert(input_ids=input_ids)\n",
    "    return output.last_hidden_state.detach().cpu().numpy()[0,0,:]\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n",
    "def get_polka_scores(text_list):\n",
    "\n",
    "    raw_text = ' '.join(text_list)\n",
    "    prompts = [\n",
    "        f\"Opinia pozytywna: {raw_text}\",\n",
    "        f\"Opinia negatywna: {raw_text}\"\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer_polka(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model_polka(input_ids=input_ids)\n",
    "            log_probs = log_probs_from_logits(output.logits[:, :-1, :], input_ids[:, 1:])\n",
    "            seq_log_prob = torch.sum(log_probs)\n",
    "\n",
    "            score = seq_log_prob / input_ids.shape[1]\n",
    "\n",
    "        scores.append(score.item())\n",
    "\n",
    "    return scores\n",
    "\n",
    "def spoil(L):\n",
    "    res = []\n",
    "    for w in L:\n",
    "        if random.random() < 0.85:\n",
    "            res.append(w)\n",
    "        else:\n",
    "            res.append(w.upper())\n",
    "    return res"
   ],
   "id": "b110eac9a610e620",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T00:04:46.353579Z",
     "start_time": "2026-02-05T00:04:46.341392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class Review:\n",
    "    positive: bool\n",
    "    content: str\n",
    "\n",
    "\n",
    "def read_reviews(path: Path) -> list[Review]:\n",
    "    reviews = []\n",
    "    with path.open() as f:\n",
    "        for line in f.readlines():\n",
    "            r_type, content = line.split(maxsplit=1)\n",
    "\n",
    "            if r_type == \"GOOD\":\n",
    "                positive = True\n",
    "            elif r_type == \"BAD\":\n",
    "                positive = False\n",
    "            else:\n",
    "                raise RuntimeError(\"Unknown review type\")\n",
    "\n",
    "            reviews.append(Review(positive=positive, content=content))\n",
    "\n",
    "    return reviews\n",
    "\n",
    "repo_root = Path(\".\").resolve().parent.parent\n",
    "reviews  = read_reviews(repo_root / \"datasets\" / \"reviews_for_task3.txt\")\n",
    "random.shuffle(reviews)\n",
    "\n",
    "N = len(reviews)\n",
    "test_size = N // 4\n",
    "train_size = N - test_size\n",
    "\n",
    "train_reviews = reviews[:train_size]\n",
    "test_reviews  = reviews[train_size:]"
   ],
   "id": "45b3dd7bf24a52f0",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T23:44:13.966468Z",
     "start_time": "2026-02-04T23:44:13.959906Z"
    }
   },
   "cell_type": "code",
   "source": "train_reviews[0], test_reviews[0]",
   "id": "d1b2ee082d8830e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Review(positive=False, content='W basenie zimna woda, w jacuzzi te≈º.\\n'),\n",
       " Review(positive=False, content='Szczerze nie polecam!!!\\n'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T00:08:36.545477Z",
     "start_time": "2026-02-05T00:04:48.389967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "print(\"Processing Training Data...\")\n",
    "for i, review in enumerate(train_reviews):\n",
    "    words = review.content.split()\n",
    "    label = 1 if review.positive else 0\n",
    "\n",
    "    polka_score = get_polka_scores(words)\n",
    "\n",
    "    bert_vec = get_bert_vect(words)\n",
    "    X_train.append(np.concatenate([bert_vec, polka_score]))\n",
    "    y_train.append(label)\n",
    "\n",
    "    for _ in range(3):\n",
    "        spoiled_words = spoil(words)\n",
    "        bert_vec_spoiled = get_bert_vect(spoiled_words)\n",
    "        # polka_score_spoiled = get_polka_scores(spoiled_words)\n",
    "        X_train.append(np.concatenate([bert_vec_spoiled, polka_score]))\n",
    "        y_train.append(label)\n",
    "\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"Train: {i + 1}/{len(train_reviews)}\")\n",
    "\n",
    "print(\"Processing Test Data...\")\n",
    "for i, review in enumerate(test_reviews):\n",
    "    words = review.content.split()\n",
    "    label = 1 if review.positive else 0\n",
    "\n",
    "    polka_score = get_polka_scores(words)\n",
    "    bert_vec = get_bert_vect(words)\n",
    "\n",
    "    X_test.append(np.concatenate([bert_vec, polka_score]))\n",
    "    y_test.append(label)\n",
    "\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"Test: {i + 1}/{len(test_reviews)}\")"
   ],
   "id": "a3de37262893d90d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Training Data...\n",
      "Train: 50/300\n",
      "Train: 100/300\n",
      "Train: 150/300\n",
      "Train: 200/300\n",
      "Train: 250/300\n",
      "Train: 300/300\n",
      "Processing Test Data...\n",
      "Test: 50/100\n",
      "Test: 100/100\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T00:08:41.896785Z",
     "start_time": "2026-02-05T00:08:41.891074Z"
    }
   },
   "cell_type": "code",
   "source": "len(X_train), len(X_test)",
   "id": "3ae4ec088373faeb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T00:08:46.485284Z",
     "start_time": "2026-02-05T00:08:46.443113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clf = LogisticRegression(max_iter=2000).fit(X_train, y_train)\n",
    "\n",
    "print ('Train accuracy:', clf.score(X_train, y_train))\n",
    "print ('Test accuracy:', clf.score(X_test, y_test))\n",
    "\n"
   ],
   "id": "b7a10c35f38a485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9991666666666666\n",
      "Test accuracy: 0.79\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8ff17d7a75db3529"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc6e74fa43647f5c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
