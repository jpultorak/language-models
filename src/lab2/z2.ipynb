{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-16T23:50:06.942675Z",
     "start_time": "2026-01-16T23:50:06.936887Z"
    }
   },
   "source": [
    "from typing import List\n",
    "from transformers import LlamaTokenizerFast\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children: dict[int, TrieNode] = {}\n",
    "        self.is_leaf = False\n",
    "\n",
    "class TokenTrie:\n",
    "    def __init__(self, tokenizer: LlamaTokenizerFast):\n",
    "        self.root = TrieNode()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def init_from_words(self, words: List[str]):\n",
    "        for word in words:\n",
    "            self.insert_word(\" \" + word)\n",
    "\n",
    "    def insert_word(self, word: str):\n",
    "        tokens = self.tokenizer.encode(word, add_special_tokens=False)\n",
    "        tokens.append(self.tokenizer.eos_token_id)\n",
    "        self._insert_tokens(tokens)\n",
    "\n",
    "    def walk(self, token_ids: List[int]):\n",
    "        cur = self.root\n",
    "        for id in token_ids:\n",
    "            if id not in cur.children:\n",
    "                raise ValueError(f\"Token {id} not in trie children at node {cur}\")\n",
    "            cur = cur.children[id]\n",
    "        return cur\n",
    "\n",
    "    def _insert_tokens(self, token_ids: List[int]):\n",
    "        cur = self.root\n",
    "        for id in token_ids:\n",
    "            if id not in cur.children:\n",
    "                cur.children[id] = TrieNode()\n",
    "            cur = cur.children[id]\n",
    "        cur.is_leaf = True"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T23:50:08.265684Z",
     "start_time": "2026-01-16T23:50:08.261602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import LogitsProcessor\n",
    "import torch\n",
    "class TrieLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, trie: TokenTrie, prompt_len: int):\n",
    "        self.trie = trie\n",
    "        self.prompt_len = prompt_len\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        masked_scores = torch.full_like(scores, -float(\"inf\"))\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            generated_tokens = input_ids[i, self.prompt_len:].tolist()\n",
    "            cur_node = self.trie.walk(generated_tokens)\n",
    "            allowed_tokens = list(cur_node.children.keys())\n",
    "\n",
    "            if not allowed_tokens:\n",
    "                raise RuntimeError(\"No possible continuation\")\n",
    "            masked_scores[i, allowed_tokens] = scores[i, allowed_tokens]\n",
    "\n",
    "        return masked_scores"
   ],
   "id": "712df17dea032e19",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T23:52:22.575637Z",
     "start_time": "2026-01-16T23:52:12.887556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"eryk-mazus/polka-1.1b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "id": "5f594c94a8c7a2f5",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T00:14:06.685752Z",
     "start_time": "2026-01-17T00:14:02.091220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "allowed_words = [\"Kraków\", \"Warszawa\", \"Londyn\", \"Stachu\", \"Opole\", \"Alutek\", \"Wawel\", \"Wieża Eiffela\", \"Monke\"]\n",
    "trie = TokenTrie(tokenizer=tokenizer)\n",
    "trie.init_from_words(allowed_words)\n",
    "\n",
    "prompt = \"Nazywa słynnego zamku w Krakowie:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(model.device)\n",
    "prompt_len = input_ids.shape[1]\n",
    "\n",
    "processor = TrieLogitsProcessor(trie, prompt_len=prompt_len)\n",
    "\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=10,\n",
    "    logits_processor=[processor],\n",
    "    num_beams=3,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(decoded)"
   ],
   "id": "e058c002e7bbdef8",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No possible continuation",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[51]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m      8\u001B[39m prompt_len = input_ids.shape[\u001B[32m1\u001B[39m]\n\u001B[32m     10\u001B[39m processor = TrieLogitsProcessor(trie, prompt_len=prompt_len)\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mprocessor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_beams\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# num_return_sequences=5,\u001B[39;49;00m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m    \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43meos_token_id\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m decoded = tokenizer.batch_decode(outputs, skip_special_tokens=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     26\u001B[39m \u001B[38;5;28mprint\u001B[39m(decoded)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/uwr/language-models/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/uwr/language-models/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001B[39m\n\u001B[32m   2561\u001B[39m model_kwargs[\u001B[33m\"\u001B[39m\u001B[33muse_cache\u001B[39m\u001B[33m\"\u001B[39m] = generation_config.use_cache\n\u001B[32m   2563\u001B[39m \u001B[38;5;66;03m# 9. Call generation mode\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2564\u001B[39m result = \u001B[43mdecoding_method\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2565\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   2566\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2567\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2568\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2569\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2570\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgeneration_mode_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2571\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2572\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2574\u001B[39m \u001B[38;5;66;03m# Convert to legacy cache format if requested\u001B[39;00m\n\u001B[32m   2575\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2576\u001B[39m     generation_config.return_legacy_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   2577\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(result, \u001B[33m\"\u001B[39m\u001B[33mpast_key_values\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   2578\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(result.past_key_values, \u001B[33m\"\u001B[39m\u001B[33mto_legacy_cache\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   2579\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/uwr/language-models/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3282\u001B[39m, in \u001B[36mGenerationMixin._beam_search\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001B[39m\n\u001B[32m   3279\u001B[39m \u001B[38;5;66;03m# b. Compute log probs -- get log probabilities from logits, process logits with processors (*e.g.*\u001B[39;00m\n\u001B[32m   3280\u001B[39m \u001B[38;5;66;03m# `temperature`, ...), and add new logprobs to existing running logprobs scores.\u001B[39;00m\n\u001B[32m   3281\u001B[39m log_probs = nn.functional.log_softmax(logits, dim=-\u001B[32m1\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m3282\u001B[39m log_probs = \u001B[43mlogits_processor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflat_running_sequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_probs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3284\u001B[39m \u001B[38;5;66;03m# Store logits, attentions and hidden_states when required\u001B[39;00m\n\u001B[32m   3285\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_dict_in_generate:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/uwr/language-models/.venv/lib/python3.13/site-packages/transformers/generation/logits_process.py:93\u001B[39m, in \u001B[36mLogitsProcessorList.__call__\u001B[39m\u001B[34m(self, input_ids, scores, **kwargs)\u001B[39m\n\u001B[32m     91\u001B[39m         scores = processor(input_ids, scores, **kwargs)\n\u001B[32m     92\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m         scores = \u001B[43mprocessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscores\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m scores\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 18\u001B[39m, in \u001B[36mTrieLogitsProcessor.__call__\u001B[39m\u001B[34m(self, input_ids, scores)\u001B[39m\n\u001B[32m     15\u001B[39m     allowed_tokens = \u001B[38;5;28mlist\u001B[39m(cur_node.children.keys())\n\u001B[32m     17\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allowed_tokens:\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mNo possible continuation\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     19\u001B[39m     masked_scores[i, allowed_tokens] = scores[i, allowed_tokens]\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m masked_scores\n",
      "\u001B[31mRuntimeError\u001B[39m: No possible continuation"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c300a6b0637f39bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
