{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T12:48:47.053635Z",
     "start_time": "2026-01-15T12:48:36.358708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessor, LogitsProcessorList\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "model_name = \"eryk-mazus/polka-1.1b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "id": "14c41c9fb3b2a78a",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T11:29:02.003911Z",
     "start_time": "2026-01-15T11:29:01.973579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(tokenizer.all_special_ids)\n",
    "\n",
    "print([tokenizer.decode([i]) for i in tokenizer.all_special_ids])"
   ],
   "id": "ec4d04e90d5c652d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 43882\n",
      "[1, 2, 0]\n",
      "['<s>', '</s>', '<unk>']\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T13:09:07.784272Z",
     "start_time": "2026-01-15T13:09:07.776873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SameLetterLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer, target_letter):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.target_letter = target_letter.lower()\n",
    "        self.separators = {\".\", \",\", \"!\", \"?\", \" \"}\n",
    "\n",
    "        self.forbidden_token_ids = []\n",
    "        self.sus_token_ids = []\n",
    "\n",
    "        vocab = tokenizer.get_vocab()\n",
    "        for token, idx in vocab.items():\n",
    "            if idx in tokenizer.all_special_ids:\n",
    "                continue\n",
    "\n",
    "            decoded = tokenizer.decode([idx])\n",
    "            if not decoded:\n",
    "                continue\n",
    "\n",
    "            starts_with_space = \"\\u2581\" in token or decoded.startswith(\" \")\n",
    "            starts_with_punct = (not decoded[0].isalpha())\n",
    "\n",
    "            letters_only = \"\".join(filter(str.isalpha, decoded))\n",
    "            counts = [decoded.count(c) for c in self.separators]\n",
    "            is_clean = all(c.isalpha() or c.isspace() or c in self.separators for c in decoded)\n",
    "\n",
    "            repeating_sep = any(cnt > 1 for cnt in counts)\n",
    "            has_sep = any(cnt > 0 for cnt in counts)\n",
    "            if (not starts_with_space and decoded[0] not in self.separators and decoded[0] and has_sep) or repeating_sep:\n",
    "                is_clean = False\n",
    "\n",
    "            if not is_clean:\n",
    "                self.forbidden_token_ids.append(idx)\n",
    "                continue\n",
    "\n",
    "            if not letters_only:\n",
    "                continue\n",
    "\n",
    "            first_char = letters_only[0].lower()\n",
    "            if first_char == self.target_letter:\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            if starts_with_punct  or starts_with_space:\n",
    "                self.forbidden_token_ids.append(idx)\n",
    "                continue\n",
    "\n",
    "            self.sus_token_ids.append(idx)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        scores[:, self.forbidden_token_ids] = -float(\"inf\")\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            last_token_id = input_ids[i, -1].item()\n",
    "            last_token_decoded = self.tokenizer.decode([last_token_id])\n",
    "            if not last_token_decoded:\n",
    "                is_boundary = True\n",
    "            else:\n",
    "                is_boundary = last_token_decoded[-1] in self.separators or last_token_decoded[-1].isspace()\n",
    "            if is_boundary:\n",
    "                scores[i, self.sus_token_ids] = -float(\"inf\")\n",
    "\n",
    "        return scores"
   ],
   "id": "11e04557fffce1fc",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T12:54:07.156807Z",
     "start_time": "2026-01-15T12:54:07.153434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate(prefix):\n",
    "    words = prefix.split()\n",
    "    target_letter = words[-1][0].lower()\n",
    "\n",
    "    processor = SameLetterLogitsProcessor(tokenizer, target_letter)\n",
    "\n",
    "    inputs = tokenizer(prefix, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=20,\n",
    "        do_sample=True,\n",
    "        top_k=100,\n",
    "        top_p=0.90,\n",
    "        repetition_penalty=1.2,\n",
    "        num_return_sequences=10,\n",
    "        logits_processor=[processor],\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    decoded_candidates = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    return decoded_candidates"
   ],
   "id": "f0f5f81e2fa4bb4a",
   "outputs": [],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T13:39:31.970761Z",
     "start_time": "2026-01-15T13:39:24.158890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prefixes = [\n",
    "    \"W wyniku wskazanych\",\n",
    "]\n",
    "\n",
    "for p in prefixes:\n",
    "    candidates = generate(p)\n",
    "    for i, c in enumerate(candidates):\n",
    "        print(f\"Candidate {i}: {c}\")"
   ],
   "id": "a3e3660708e93659",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 0: W wyniku wskazanych wyżej wpadek w w wejściu, wieży, wieżowiec, włącz\n",
      "Candidate 1: W wyniku wskazanych wyżej wskaźników wzrostu, wynagrodzenie wskoczyło w tymże,\n",
      "Candidate 2: W wyniku wskazanych wniosków, wojewoda wielkopolski wydał w dniu wydania w dniach ww. \n",
      "Candidate 3: W wyniku wskazanych wskaźników wzrostowych, webshop . Ważккѕkсyкк\n",
      "Candidate 4: W wyniku wskazanych wyżej wad, wpływających wprost wpłynęło w dalszym \n",
      "Candidate 5: W wyniku wskazanych wymogów,  w wielu większych wymianach wielu\n",
      "Candidate 6: W wyniku wskazanych wyżej względów, wszelkie wypłaty wygranej w wersji wirtual\n",
      "Candidate 7: W wyniku wskazanych wyżej względów, wszelkie wymienione we wniosku warunki warunek \n",
      "Candidate 8: W wyniku wskazanych wyżej wytycznych wartość wywołania wartościowej wersji witryny\n",
      "Candidate 9: W wyniku wskazanych wniosków wydano w dalszym . Ważńciu Wojewody Warmińsko\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c9844bf9048ca59"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
