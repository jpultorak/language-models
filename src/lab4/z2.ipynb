{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cute little demo showing the simplest usage of minGPT. Configured to run fine on Macbook Air in like a minute."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T12:07:06.785794Z",
     "start_time": "2026-01-23T12:07:05.601882Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.utils import set_seed\n",
    "\n",
    "set_seed(3407)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T12:07:06.797907Z",
     "start_time": "2026-01-23T12:07:06.793548Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "def mult_digit_by_digit(val_a, b_digits):\n",
    "    res = []\n",
    "    mul = 1\n",
    "    for digit in reversed(b_digits):\n",
    "        res.append(val_a * digit * mul)\n",
    "        mul *= 10\n",
    "\n",
    "    res.append(sum(res))\n",
    "    return res\n",
    "\n",
    "def pad_number(a, need_len):\n",
    "    str_a = str(a)\n",
    "    pad = need_len - len(str_a)\n",
    "    str_a = pad * '0' + str_a\n",
    "    return str_a\n",
    "\n",
    "def random_mult_instance(length):\n",
    "    a = [random.randint(0,9) for i in range(length)]\n",
    "    b = [random.randint(0,9) for i in range(length)]\n",
    "\n",
    "    val_a = int(''.join(str(d) for d in a))\n",
    "\n",
    "    comp = mult_digit_by_digit(val_a, b)\n",
    "    need_len = 2*length\n",
    "\n",
    "    comp = [pad_number(x, need_len) for x in comp]\n",
    "    return a + b + [int(d) for s in comp for d in s]\n",
    "\n",
    "random_mult_instance(2)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 6, 2, 5, 0, 0, 3, 0, 0, 1, 2, 0, 0, 1, 5, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T12:07:19.114688Z",
     "start_time": "2026-01-23T12:07:19.110068Z"
    }
   },
   "source": [
    "class MultDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset for the Add problem. E.g. for problem length 3:\n",
    "    12 + 333 = 345\n",
    "    Input: 0 1 2 3 3 3 -> Output: 0 3 4 5\n",
    "    Which will feed into the transformer concatenated as:\n",
    "    input:  0 1 2 3 3 3 0 3 4\n",
    "    output: I I I I I 0 3 4 5\n",
    "    where I is \"ignore\", as the transformer is reading the input sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=3):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 10000 # ...\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return 10\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        return 2*self.length + (self.length + 1) * (2 * self.length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            rai = random_mult_instance(self.length)\n",
    "            h = hash(str(rai[:2*self.length]))\n",
    "            \n",
    "            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n",
    "            if inp_split == self.split:\n",
    "                break # ok\n",
    "        \n",
    "        x = torch.tensor(rai[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(rai[1:], dtype=torch.long)\n",
    "        \n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[:2*self.length-1] = -1\n",
    "        return x, y"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T12:07:21.027637Z",
     "start_time": "2026-01-23T12:07:21.018818Z"
    }
   },
   "source": [
    "# print an example instance of the dataset\n",
    "train_dataset = MultDataset('train')\n",
    "test_dataset = MultDataset('test')\n",
    "x, y = train_dataset[0]\n",
    "\n",
    "print (x)\n",
    "for a, b in zip(x,y):\n",
    "    print(int(a),int(b))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 6, 4, 9, 5, 3, 0, 0, 1, 0, 9, 2, 0, 1, 8, 2, 0, 0, 3, 2, 7, 6, 0, 0,\n",
      "        3, 4, 6, 8, 9])\n",
      "3 -1\n",
      "6 -1\n",
      "4 -1\n",
      "9 -1\n",
      "5 -1\n",
      "3 0\n",
      "0 0\n",
      "0 1\n",
      "1 0\n",
      "0 9\n",
      "9 2\n",
      "2 0\n",
      "0 1\n",
      "1 8\n",
      "8 2\n",
      "2 0\n",
      "0 0\n",
      "0 3\n",
      "3 2\n",
      "2 7\n",
      "7 6\n",
      "6 0\n",
      "0 0\n",
      "0 3\n",
      "3 4\n",
      "4 6\n",
      "6 8\n",
      "8 9\n",
      "9 2\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T12:07:26.145731Z",
     "start_time": "2026-01-23T12:07:26.109645Z"
    }
   },
   "source": [
    "# create a GPT instance\n",
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-micro'\n",
    "#model_config.model_type = 'gpt-nano'\n",
    "\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model = GPT(model_config)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.80M\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T12:07:27.196900Z",
     "start_time": "2026-01-23T12:07:27.194316Z"
    }
   },
   "source": [
    "print (model_config.n_head, model_config.n_layer, model_config.n_embd)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 128\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T12:07:28.051218Z",
     "start_time": "2026-01-23T12:07:28.044378Z"
    }
   },
   "source": [
    "# create a Trainer object\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 30000\n",
    "train_config.num_workers = 0\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T12:37:58.754454Z",
     "start_time": "2026-01-23T12:07:29.557831Z"
    }
   },
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janek/dev/uwr/language-models/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 2.36803\n",
      "iter_dt 87.19ms; iter 100: train loss 1.28014\n",
      "iter_dt 115.63ms; iter 200: train loss 1.08944\n",
      "iter_dt 118.49ms; iter 300: train loss 0.91356\n",
      "iter_dt 116.62ms; iter 400: train loss 0.76703\n",
      "iter_dt 119.54ms; iter 500: train loss 0.60055\n",
      "iter_dt 115.96ms; iter 600: train loss 0.56211\n",
      "iter_dt 112.88ms; iter 700: train loss 0.48590\n",
      "iter_dt 112.57ms; iter 800: train loss 0.43079\n",
      "iter_dt 105.40ms; iter 900: train loss 0.43017\n",
      "iter_dt 104.44ms; iter 1000: train loss 0.39121\n",
      "iter_dt 111.30ms; iter 1100: train loss 0.32052\n",
      "iter_dt 116.56ms; iter 1200: train loss 0.28754\n",
      "iter_dt 104.58ms; iter 1300: train loss 0.25755\n",
      "iter_dt 116.30ms; iter 1400: train loss 0.22159\n",
      "iter_dt 110.39ms; iter 1500: train loss 0.21877\n",
      "iter_dt 111.02ms; iter 1600: train loss 0.18469\n",
      "iter_dt 110.03ms; iter 1700: train loss 0.19993\n",
      "iter_dt 114.95ms; iter 1800: train loss 0.17058\n",
      "iter_dt 108.41ms; iter 1900: train loss 0.14142\n",
      "iter_dt 108.85ms; iter 2000: train loss 0.15757\n",
      "iter_dt 109.13ms; iter 2100: train loss 0.13539\n",
      "iter_dt 117.25ms; iter 2200: train loss 0.13444\n",
      "iter_dt 115.28ms; iter 2300: train loss 0.13597\n",
      "iter_dt 110.65ms; iter 2400: train loss 0.16761\n",
      "iter_dt 114.65ms; iter 2500: train loss 0.13515\n",
      "iter_dt 116.81ms; iter 2600: train loss 0.10349\n",
      "iter_dt 108.64ms; iter 2700: train loss 0.11319\n",
      "iter_dt 107.31ms; iter 2800: train loss 0.09194\n",
      "iter_dt 120.86ms; iter 2900: train loss 0.08739\n",
      "iter_dt 109.59ms; iter 3000: train loss 0.08506\n",
      "iter_dt 114.32ms; iter 3100: train loss 0.08229\n",
      "iter_dt 113.16ms; iter 3200: train loss 0.08538\n",
      "iter_dt 98.26ms; iter 3300: train loss 0.07624\n",
      "iter_dt 111.82ms; iter 3400: train loss 0.08681\n",
      "iter_dt 124.70ms; iter 3500: train loss 0.07368\n",
      "iter_dt 111.84ms; iter 3600: train loss 0.09503\n",
      "iter_dt 119.71ms; iter 3700: train loss 0.06815\n",
      "iter_dt 113.67ms; iter 3800: train loss 0.08038\n",
      "iter_dt 113.03ms; iter 3900: train loss 0.08358\n",
      "iter_dt 113.85ms; iter 4000: train loss 0.07311\n",
      "iter_dt 130.36ms; iter 4100: train loss 0.06316\n",
      "iter_dt 114.66ms; iter 4200: train loss 0.07741\n",
      "iter_dt 117.31ms; iter 4300: train loss 0.06439\n",
      "iter_dt 110.65ms; iter 4400: train loss 0.06072\n",
      "iter_dt 108.52ms; iter 4500: train loss 0.06049\n",
      "iter_dt 118.41ms; iter 4600: train loss 0.05973\n",
      "iter_dt 108.93ms; iter 4700: train loss 0.05578\n",
      "iter_dt 141.95ms; iter 4800: train loss 0.06736\n",
      "iter_dt 108.22ms; iter 4900: train loss 0.06969\n",
      "iter_dt 117.55ms; iter 5000: train loss 0.05253\n",
      "iter_dt 111.89ms; iter 5100: train loss 0.06180\n",
      "iter_dt 106.15ms; iter 5200: train loss 0.05039\n",
      "iter_dt 97.46ms; iter 5300: train loss 0.06293\n",
      "iter_dt 108.55ms; iter 5400: train loss 0.06292\n",
      "iter_dt 119.88ms; iter 5500: train loss 0.06103\n",
      "iter_dt 101.90ms; iter 5600: train loss 0.06043\n",
      "iter_dt 116.76ms; iter 5700: train loss 0.06005\n",
      "iter_dt 119.40ms; iter 5800: train loss 0.05480\n",
      "iter_dt 110.62ms; iter 5900: train loss 0.05020\n",
      "iter_dt 117.81ms; iter 6000: train loss 0.03948\n",
      "iter_dt 111.70ms; iter 6100: train loss 0.05306\n",
      "iter_dt 113.43ms; iter 6200: train loss 0.04265\n",
      "iter_dt 108.67ms; iter 6300: train loss 0.05389\n",
      "iter_dt 121.94ms; iter 6400: train loss 0.04947\n",
      "iter_dt 127.12ms; iter 6500: train loss 0.04811\n",
      "iter_dt 122.96ms; iter 6600: train loss 0.04497\n",
      "iter_dt 109.44ms; iter 6700: train loss 0.05228\n",
      "iter_dt 119.51ms; iter 6800: train loss 0.03138\n",
      "iter_dt 117.82ms; iter 6900: train loss 0.04076\n",
      "iter_dt 124.72ms; iter 7000: train loss 0.04684\n",
      "iter_dt 118.25ms; iter 7100: train loss 0.04394\n",
      "iter_dt 109.89ms; iter 7200: train loss 0.04569\n",
      "iter_dt 113.20ms; iter 7300: train loss 0.03870\n",
      "iter_dt 124.51ms; iter 7400: train loss 0.04861\n",
      "iter_dt 120.21ms; iter 7500: train loss 0.03531\n",
      "iter_dt 114.59ms; iter 7600: train loss 0.03047\n",
      "iter_dt 116.93ms; iter 7700: train loss 0.04264\n",
      "iter_dt 119.89ms; iter 7800: train loss 0.04657\n",
      "iter_dt 108.27ms; iter 7900: train loss 0.05671\n",
      "iter_dt 138.84ms; iter 8000: train loss 0.03469\n",
      "iter_dt 131.22ms; iter 8100: train loss 0.05034\n",
      "iter_dt 126.89ms; iter 8200: train loss 0.03783\n",
      "iter_dt 114.51ms; iter 8300: train loss 0.03061\n",
      "iter_dt 96.07ms; iter 8400: train loss 0.03423\n",
      "iter_dt 115.87ms; iter 8500: train loss 0.02781\n",
      "iter_dt 106.33ms; iter 8600: train loss 0.05047\n",
      "iter_dt 115.99ms; iter 8700: train loss 0.05072\n",
      "iter_dt 130.92ms; iter 8800: train loss 0.03567\n",
      "iter_dt 117.89ms; iter 8900: train loss 0.03180\n",
      "iter_dt 114.97ms; iter 9000: train loss 0.04061\n",
      "iter_dt 104.15ms; iter 9100: train loss 0.02579\n",
      "iter_dt 119.44ms; iter 9200: train loss 0.03018\n",
      "iter_dt 91.94ms; iter 9300: train loss 0.04926\n",
      "iter_dt 114.43ms; iter 9400: train loss 0.03327\n",
      "iter_dt 120.72ms; iter 9500: train loss 0.02766\n",
      "iter_dt 111.79ms; iter 9600: train loss 0.03980\n",
      "iter_dt 114.11ms; iter 9700: train loss 0.04309\n",
      "iter_dt 118.11ms; iter 9800: train loss 0.03070\n",
      "iter_dt 111.07ms; iter 9900: train loss 0.02744\n",
      "iter_dt 125.78ms; iter 10000: train loss 0.02424\n",
      "iter_dt 111.96ms; iter 10100: train loss 0.03781\n",
      "iter_dt 119.14ms; iter 10200: train loss 0.02502\n",
      "iter_dt 136.12ms; iter 10300: train loss 0.01994\n",
      "iter_dt 112.22ms; iter 10400: train loss 0.02379\n",
      "iter_dt 111.43ms; iter 10500: train loss 0.02959\n",
      "iter_dt 111.41ms; iter 10600: train loss 0.03865\n",
      "iter_dt 115.92ms; iter 10700: train loss 0.03729\n",
      "iter_dt 116.59ms; iter 10800: train loss 0.03166\n",
      "iter_dt 128.30ms; iter 10900: train loss 0.03181\n",
      "iter_dt 87.03ms; iter 11000: train loss 0.02467\n",
      "iter_dt 117.66ms; iter 11100: train loss 0.02315\n",
      "iter_dt 124.87ms; iter 11200: train loss 0.02389\n",
      "iter_dt 123.90ms; iter 11300: train loss 0.02046\n",
      "iter_dt 110.76ms; iter 11400: train loss 0.03416\n",
      "iter_dt 116.26ms; iter 11500: train loss 0.02698\n",
      "iter_dt 106.41ms; iter 11600: train loss 0.02956\n",
      "iter_dt 124.20ms; iter 11700: train loss 0.02746\n",
      "iter_dt 111.81ms; iter 11800: train loss 0.03096\n",
      "iter_dt 128.67ms; iter 11900: train loss 0.02683\n",
      "iter_dt 116.03ms; iter 12000: train loss 0.02833\n",
      "iter_dt 123.11ms; iter 12100: train loss 0.04945\n",
      "iter_dt 123.36ms; iter 12200: train loss 0.02462\n",
      "iter_dt 125.61ms; iter 12300: train loss 0.02378\n",
      "iter_dt 114.18ms; iter 12400: train loss 0.02893\n",
      "iter_dt 121.19ms; iter 12500: train loss 0.02331\n",
      "iter_dt 93.77ms; iter 12600: train loss 0.01610\n",
      "iter_dt 123.45ms; iter 12700: train loss 0.02434\n",
      "iter_dt 110.91ms; iter 12800: train loss 0.03045\n",
      "iter_dt 134.73ms; iter 12900: train loss 0.02873\n",
      "iter_dt 109.05ms; iter 13000: train loss 0.01203\n",
      "iter_dt 120.99ms; iter 13100: train loss 0.01588\n",
      "iter_dt 125.25ms; iter 13200: train loss 0.01491\n",
      "iter_dt 112.59ms; iter 13300: train loss 0.03628\n",
      "iter_dt 109.48ms; iter 13400: train loss 0.01855\n",
      "iter_dt 117.34ms; iter 13500: train loss 0.02712\n",
      "iter_dt 127.33ms; iter 13600: train loss 0.01554\n",
      "iter_dt 117.49ms; iter 13700: train loss 0.02016\n",
      "iter_dt 111.18ms; iter 13800: train loss 0.02107\n",
      "iter_dt 110.68ms; iter 13900: train loss 0.01530\n",
      "iter_dt 122.67ms; iter 14000: train loss 0.01615\n",
      "iter_dt 127.58ms; iter 14100: train loss 0.01837\n",
      "iter_dt 103.77ms; iter 14200: train loss 0.01384\n",
      "iter_dt 119.67ms; iter 14300: train loss 0.01416\n",
      "iter_dt 124.46ms; iter 14400: train loss 0.02399\n",
      "iter_dt 113.06ms; iter 14500: train loss 0.01399\n",
      "iter_dt 123.99ms; iter 14600: train loss 0.01724\n",
      "iter_dt 104.33ms; iter 14700: train loss 0.01827\n",
      "iter_dt 85.30ms; iter 14800: train loss 0.02211\n",
      "iter_dt 119.26ms; iter 14900: train loss 0.02268\n",
      "iter_dt 86.68ms; iter 15000: train loss 0.03269\n",
      "iter_dt 116.67ms; iter 15100: train loss 0.01437\n",
      "iter_dt 105.96ms; iter 15200: train loss 0.03009\n",
      "iter_dt 104.59ms; iter 15300: train loss 0.01015\n",
      "iter_dt 107.09ms; iter 15400: train loss 0.02282\n",
      "iter_dt 131.42ms; iter 15500: train loss 0.02780\n",
      "iter_dt 112.48ms; iter 15600: train loss 0.02181\n",
      "iter_dt 116.42ms; iter 15700: train loss 0.02194\n",
      "iter_dt 118.40ms; iter 15800: train loss 0.01687\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      3\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33miter_dt \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer.iter_dt\u001B[38;5;250m \u001B[39m*\u001B[38;5;250m \u001B[39m\u001B[32m1000\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33mms; iter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer.iter_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: train loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer.loss.item()\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.5f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m      4\u001B[39m trainer.set_callback(\u001B[33m'\u001B[39m\u001B[33mon_batch_end\u001B[39m\u001B[33m'\u001B[39m, batch_end_callback)\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/uwr/language-models/.venv/lib/python3.13/site-packages/mingpt/trainer.py:97\u001B[39m, in \u001B[36mTrainer.run\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[38;5;66;03m# backprop and update the parameters\u001B[39;00m\n\u001B[32m     96\u001B[39m model.zero_grad(set_to_none=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     98\u001B[39m torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n\u001B[32m     99\u001B[39m \u001B[38;5;28mself\u001B[39m.optimizer.step()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/uwr/language-models/.venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/uwr/language-models/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/uwr/language-models/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T12:38:02.285849Z",
     "start_time": "2026-01-23T12:38:02.283064Z"
    }
   },
   "source": [
    "# now let's perform some evaluation\n",
    "model.eval()\n",
    "None"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T12:40:59.188401Z",
     "start_time": "2026-01-23T12:38:03.948519Z"
    }
   },
   "source": [
    "def eval_add_split(trainer, split, max_batches):\n",
    "    dataset = {'train':train_dataset, 'test':test_dataset}[split]\n",
    "    n = train_dataset.length # naugy direct access shrug\n",
    "    results = []\n",
    "    mistakes_printed_already = 0\n",
    "    loader = DataLoader(dataset, batch_size=100, num_workers=0, drop_last=False)\n",
    "\n",
    "    #loader = DataLoader(dataset, batch_size=1, num_workers=0, drop_last=False)\n",
    "    for b, (x, y) in enumerate(loader):\n",
    "        x = x.to(trainer.device)\n",
    "        y = y.to(trainer.device)\n",
    "\n",
    "        inp = x[:, :2*n]\n",
    "        sol = y[:, -2*n:]\n",
    "        \n",
    "        cat = model.generate(inp, (n+1)*2*n, do_sample=False) # using greedy argmax, not sampling\n",
    "        sol_candidate = cat[:, -2*n:]\n",
    "        correct = (sol == sol_candidate).all(1).cpu() \n",
    "        for i in range(x.size(0)):\n",
    "            results.append(int(correct[i]))\n",
    "    \n",
    "    rt = torch.tensor(results, dtype=torch.float)\n",
    "    print(\"%s final score: %d/%d = %.2f%% correct\" % (split, rt.sum(), len(results), 100*rt.mean()))\n",
    "    return rt.sum()\n",
    "\n",
    "# run a lot of examples from both train and test through the model and verify the output correctness\n",
    "with torch.no_grad():\n",
    "    train_score = eval_add_split(trainer, 'train', max_batches=50)\n",
    "    test_score  = eval_add_split(trainer, 'test',  max_batches=50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train final score: 9739/10000 = 97.39% correct\n",
      "test final score: 9744/10000 = 97.44% correct\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
